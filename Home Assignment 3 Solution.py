# -*- coding: utf-8 -*-
"""HomeAssignment3_Solution.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UPB8x_tW9JnE_CPZBfNvSUpPC-D3OqsM

Home Assignment - 3

Student Name: SAI SRAVAN CHINTALA

Student ID: 700773836

Q1: Implementing an RNN for Text Generation

Task: Recurrent Neural Networks (RNNs) can generate sequences of text. You will train an LSTM-based RNN to predict the next character in a given text dataset.

1.	Load a text dataset (e.g., "Shakespeare Sonnets", "The Little Prince").
2.	Convert text into a sequence of characters (one-hot encoding or embeddings).
3.	Define an RNN model using LSTM layers to predict the next character.
4.	Train the model and generate new text by sampling characters one at a time.
5.	Explain the role of temperature scaling in text generation and its effect on randomness.

Hint: Use tensorflow.keras.layers.LSTM() for sequence modeling.
"""

import tensorflow as tf
import numpy as np
import os

# Load the text dataset
path_to_file = tf.keras.utils.get_file("shakespeare.txt", "https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt")
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')

print(f"Length of text: {len(text)} characters")
print(f"First 250 characters:\n{text[:250]}")

# Create a mapping from characters to indices
vocab = sorted(set(text))  # Unique characters
char2idx = {char: idx for idx, char in enumerate(vocab)}
idx2char = np.array(vocab)

# Convert text to integers
text_as_int = np.array([char2idx[char] for char in text])

# Check the mapping
print(f"Vocabulary size: {len(vocab)}")
print(f"Example mapping: {char2idx}")

import tensorflow as tf
import numpy as np
import os

# Load the text dataset
path_to_file = tf.keras.utils.get_file("shakespeare.txt", "https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt")
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')

print(f"Length of text: {len(text)} characters")
print(f"First 250 characters:\n{text[:250]}")

# Create a mapping from characters to indices
vocab = sorted(set(text))  # Unique characters
char2idx = {char: idx for idx, char in enumerate(vocab)}
idx2char = np.array(vocab)

# Convert text to integers
text_as_int = np.array([char2idx[char] for char in text])

# Check the mapping
print(f"Vocabulary size: {len(vocab)}")
print(f"Example mapping: {char2idx}")

# Create sequences for training
seq_length = 100  # Sequence length
examples_per_epoch = len(text) // seq_length

# Create dataset
char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)

# Generate sequences
sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)

# Split input and target text
def split_input_target(chunk):
    input_text = chunk[:-1]
    target_text = chunk[1:]
    return input_text, target_text

dataset = sequences.map(split_input_target)

# Shuffle and batch the dataset
BATCH_SIZE = 64
BUFFER_SIZE = 10000
dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

# Define the model
vocab_size = len(vocab)
embedding_dim = 256
rnn_units = 1024

def build_model(vocab_size, embedding_dim, rnn_units, batch_size):
    model = tf.keras.Sequential([
        # Removed batch_input_shape from the Embedding layer
        tf.keras.layers.Embedding(vocab_size, embedding_dim),
        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),
        tf.keras.layers.Dense(vocab_size)
    ])
    return model

model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)

# Loss function
def loss(labels, logits):
    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)

model.compile(optimizer='adam', loss=loss)

# Training
EPOCHS = 10
history = model.fit(dataset, epochs=EPOCHS)

import tensorflow as tf
import numpy as np
import os

# Load the text dataset
path_to_file = tf.keras.utils.get_file("shakespeare.txt", "https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt")
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')

print(f"Length of text: {len(text)} characters")
print(f"First 250 characters:\n{text[:250]}")

# Create a mapping from characters to indices
vocab = sorted(set(text))  # Unique characters
char2idx = {char: idx for idx, char in enumerate(vocab)}
idx2char = np.array(vocab)

# Convert text to integers
text_as_int = np.array([char2idx[char] for char in text])

# Check the mapping
print(f"Vocabulary size: {len(vocab)}")
print(f"Example mapping: {char2idx}")

# Create sequences for training
seq_length = 100  # Sequence length
examples_per_epoch = len(text) // seq_length

# Create dataset
char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)

# Generate sequences
sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)

# Split input and target text
def split_input_target(chunk):
    input_text = chunk[:-1]
    target_text = chunk[1:]
    return input_text, target_text

dataset = sequences.map(split_input_target)

# Shuffle and batch the dataset
BATCH_SIZE = 64
BUFFER_SIZE = 10000
dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

# Define the model
vocab_size = len(vocab)
embedding_dim = 256
rnn_units = 1024

def build_model(vocab_size, embedding_dim, rnn_units, batch_size):
    model = tf.keras.Sequential([
        # Removed batch_input_shape from the Embedding layer
        tf.keras.layers.Embedding(vocab_size, embedding_dim), # Removed batch_input_shape
        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),
        tf.keras.layers.Dense(vocab_size)
    ])
    return model

model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)

"""*5. Explain the role of temperature scaling in text generation and its effect on randomness.*

**Answer:** It controls the randomness of predictions during text generation.

Higher temperatures (>1) make the predictions more random.

Lower temperatures (<1) make the predictions more deterministic.

Effect on Text Generation:

A high temperature generates more creative and diverse text but can include incoherent results.

A low temperature generates predictable text but might lack diversity.

Q2: NLP Preprocessing Pipeline

Write a Python function that performs basic NLP preprocessing on a sentence. The function should do the following steps:
1.	Tokenize the sentence into individual words.
2.	Remove common English stopwords (e.g., "the", "in", "are").
3.	Apply stemming to reduce each word to its root form.
Use the sentence: "NLP techniques are used in virtual assistants like Alexa and Siri."

The function should print:
•	A list of all tokens
•	The list after stop words are removed
•	The final list after stemming

Expected Output:

Your program should print three outputs in order:
1.	Original Tokens – All words and punctuation split from the sentence
2.	Tokens Without Stopwords – Only meaningful words remain
3.	Stemmed Words – Each word is reduced to its base/root form
"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
# Download the required punkt_tab resource
nltk.download('punkt_tab')


def nlp_preprocessing(sentence):
    # Step 1: Tokenize the sentence
    tokens = word_tokenize(sentence)
    print("Original Tokens:", tokens)

    # Step 2: Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens_without_stopwords = [word for word in tokens if word.lower() not in stop_words]
    print("Tokens Without Stopwords:", tokens_without_stopwords)

    # Step 3: Apply stemming
    stemmer = PorterStemmer()
    stemmed_tokens = [stemmer.stem(word) for word in tokens_without_stopwords]
    print("Stemmed Words:", stemmed_tokens)

# Input sentence
sentence = "NLP techniques are used in virtual assistants like Alexa and Siri."
nlp_preprocessing(sentence)

"""**Short Answer Questions:**

*1.	What is the difference between stemming and lemmatization? Provide examples with the word “running.”*

**Answer:** Difference Between Stemming and Lemmatization
Stemming:

Reduces words to their root form by chopping off prefixes or suffixes using a set of rules.

Often results in non-standard words.

Example with "running":

Stemmed word: run

Lemmatization:

Reduces words to their base form (lemma) based on vocabulary and grammar rules.

Always produces meaningful words.

Example with "running":

Lemmatized word: run

*2.	Why might removing stop words be useful in some NLP tasks, and when might it actually be harmful?*

**Answer:**

Usefulness in NLP Tasks:

Stop words like "the," "and," or "in" are common and often do not add meaningful information for tasks like text classification or information retrieval.

Removing them can reduce dimensionality and improve computational efficiency.

When It Can Be Harmful:

In tasks like sentiment analysis or language modeling, stop words may carry contextual or emotional significance.

Example: In the sentence "I do not like this," the word "not" is a stop word but is crucial for understanding the sentiment.

Q3: Named Entity Recognition with SpaCy

Task: Use the spaCy library to extract named entities from a sentence. For each entity, print:

•	The entity text (e.g., "Barack Obama")
•	The entity label (e.g., PERSON, DATE)
•	The start and end character positions in the string

Use the input sentence: "Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009."

Expected Output: Each line of the output should describe one entity detected
"""

import spacy

# Load SpaCy's pre-trained English model
nlp = spacy.load("en_core_web_sm")

# Input sentence
sentence = "Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009."

# Process the sentence with SpaCy
doc = nlp(sentence)

# Extract and print named entities
print("Named Entities:")
for ent in doc.ents:
    print(f"Text: {ent.text}, Label: {ent.label_}, Start: {ent.start_char}, End: {ent.end_char}")

"""**Short Answer Questions:**

*1.	How does NER differ from POS tagging in NLP?*

**Answer:**

Named Entity Recognition (NER): Identifies specific entities in text, such as names, dates, locations, organizations, etc.

Example: In the sentence "Barack Obama is the 44th President," NER identifies "Barack Obama" as a PERSON and "44th" as an ORDINAL.

Part-of-Speech (POS) Tagging: Assigns grammatical labels to words based on their role in the sentence (e.g., noun, verb, adjective).

Example: In the same sentence, POS tagging identifies "Barack" as a PROPN (proper noun), "is" as a VERB, and "President" as a NOUN.

Key Difference:
NER focuses on semantic meaning (e.g., who or what the entity is), while POS tagging deals with syntactic roles (e.g., how words function in a sentence).

*2.	Describe two applications that use NER in the real world (e.g., financial news, search engines).*

**Answer:**
1. Financial News Analysis: Extracts company names, stock symbols, dates, and financial events from news articles.

* Used in applications like algorithmic trading or financial forecasting.

2. Search Engines: Enhances search relevance by identifying entities in user queries.

Example: For a query like "Best hotels in Paris," the search engine detects "Paris" as a location (GPE) and retrieves location-specific results.

Q4: Scaled Dot-Product Attention

Task: Implement the scaled dot-product attention mechanism. Given matrices Q (Query), K (Key), and V (Value), your function should:

*	Compute the dot product of Q and Kᵀ

*	Scale the result by dividing it by √d (where d is the key dimension)

*	Apply softmax to get attention weights

*	Multiply the weights by V to get the output

Use the following test inputs:

Q = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])

K = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])

V = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])

Expected Output Description: Your output should display:
1.	The attention weights matrix (after softmax)
2.	The final output matrix
"""

import numpy as np

def scaled_dot_product_attention(Q, K, V):
    # Step 1: Compute dot product of Q and Kᵀ
    d_k = Q.shape[-1]  # Dimension of the keys
    scores = np.dot(Q, K.T)

    # Step 2: Scale the scores by √d
    scaled_scores = scores / np.sqrt(d_k)

    # Step 3: Apply softmax to get attention weights
    attention_weights = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores), axis=-1, keepdims=True)

    # Step 4: Multiply the attention weights by V to get the output
    output = np.dot(attention_weights, V)

    return attention_weights, output

# Test inputs
Q = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])
K = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])
V = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])

# Compute attention weights and output
attention_weights, output = scaled_dot_product_attention(Q, K, V)

# Print results
print("Attention Weights Matrix:")
print(attention_weights)

print("\nOutput Matrix:")
print(output)

"""**Short Answer Questions:**

*1.	Why do we divide the attention score by √d in the scaled dot-product attention formula?*

**Answer:**

To prevent large values in the dot product:

* The dot product of Q and K can grow large as the dimensionality (d𝑘​) increases.

* This can lead to very large values in the softmax function, which may saturate to produce extremely small gradients.

Scaling by √d𝑘 :
* Normalizes the scores, ensuring numerical stability and preventing vanishing/exploding gradients.

* This helps the model learn more effectively.

*2. How does self-attention help the model understand relationships between words in a sentence?*

**Answer:**

Contextual Relationships:

* Self-attention allows each word in a sentence to attend to all other words, capturing relationships regardless of distance.

* Example: In the sentence "The cat sat on the mat," self-attention can help the model understand that "cat" is related to "sat."

Dynamic Representations:

* Each word's representation is updated dynamically based on its relevance to other words in the sequence.

* This is critical for tasks like translation, where the meaning of a word depends on its context.

Q5: Sentiment Analysis using HuggingFace Transformers

Task: Use the HuggingFace transformers library to create a sentiment classifier. Your program should:
*	Load a pre-trained sentiment analysis pipeline
*	Analyze the following input sentence:
"Despite the high price, the performance of the new MacBook is outstanding."

*	Print:
**	Label (e.g., POSITIVE, NEGATIVE)
**  Confidence score (e.g., 0.9985)

Expected Output:

Your output should clearly display:

Sentiment: [Label]

Confidence Score: [Decimal between 0 and 1]
"""

from transformers import pipeline

# Load the pre-trained sentiment analysis pipeline
sentiment_pipeline = pipeline("sentiment-analysis")

# Input sentence
sentence = "Despite the high price, the performance of the new MacBook is outstanding."

# Analyze sentiment
result = sentiment_pipeline(sentence)[0]

# Print sentiment and confidence score
print(f"Sentiment: {result['label']}")
print(f"Confidence Score: {result['score']:.4f}")

"""**Short Answer Questions:**

*1.	What is the main architectural difference between BERT and GPT? Which uses an encoder and which uses a decoder?*

**Answer:**

BERT (Bidirectional Encoder Representations from Transformers):

* Uses the encoder portion of the Transformer architecture.

* Focuses on understanding the context of a word by looking at both its left and right neighbors in the text.

* Trained with tasks like masked language modeling (predicting masked tokens) and next sentence prediction.

GPT (Generative Pre-trained Transformer):

* Uses the decoder portion of the Transformer architecture.

* Operates in an autoregressive manner, predicting the next token in a sequence based on the previous tokens.

* Focuses more on text generation tasks.

Key Difference:

* BERT is designed for understanding text (e.g., classification, QA), while GPT is optimized for generating text (e.g., writing essays, stories).




*2.	Explain why using pre-trained models (like BERT or GPT) is beneficial for NLP applications instead of training from scratch.*

**Answer:**

* Time and Resource Efficiency:

Training a large language model from scratch requires enormous computational
resources, time, and data. Pre-trained models save these costs.

* Transfer Learning:

Pre-trained models have already learned language representations from massive corpora.

Fine-tuning these models on specific tasks allows you to leverage their prior knowledge with minimal additional training.

* Improved Performance:

Pre-trained models often achieve state-of-the-art results in various NLP tasks due to their ability to generalize across tasks.

* Accessibility:

Libraries like HuggingFace make these models easily accessible, allowing even small teams to deploy cutting-edge NLP systems.
"""